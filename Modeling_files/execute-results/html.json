{
  "hash": "5c1551c34774da14b32399555472ce76",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Modeling\"\nformat: html\neditor: visual\neditor_options: \n  chunk_output_type: console\n---\n\n\n## Introduction\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#load packages needed\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'tidyverse' was built under R version 4.3.3\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'ggplot2' was built under R version 4.3.3\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'readr' was built under R version 4.3.3\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(caret)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'caret' was built under R version 4.3.3\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: lattice\n\nAttaching package: 'caret'\n\nThe following object is masked from 'package:purrr':\n\n    lift\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(Metrics)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'Metrics' was built under R version 4.3.3\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'Metrics'\n\nThe following objects are masked from 'package:caret':\n\n    precision, recall\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(rpart)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'rpart' was built under R version 4.3.3\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(ranger)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'ranger' was built under R version 4.3.3\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#read in csv data\ndata_original <- read_csv('diabetes_binary_health_indicators_BRFSS2015.csv')\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 253680 Columns: 22\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (22): Diabetes_binary, HighBP, HighChol, CholCheck, BMI, Smoker, Stroke,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n\n```{.r .cell-code}\nstr(data_original)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nspc_tbl_ [253,680 × 22] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ Diabetes_binary     : num [1:253680] 0 0 0 0 0 0 0 0 1 0 ...\n $ HighBP              : num [1:253680] 1 0 1 1 1 1 1 1 1 0 ...\n $ HighChol            : num [1:253680] 1 0 1 0 1 1 0 1 1 0 ...\n $ CholCheck           : num [1:253680] 1 0 1 1 1 1 1 1 1 1 ...\n $ BMI                 : num [1:253680] 40 25 28 27 24 25 30 25 30 24 ...\n $ Smoker              : num [1:253680] 1 1 0 0 0 1 1 1 1 0 ...\n $ Stroke              : num [1:253680] 0 0 0 0 0 0 0 0 0 0 ...\n $ HeartDiseaseorAttack: num [1:253680] 0 0 0 0 0 0 0 0 1 0 ...\n $ PhysActivity        : num [1:253680] 0 1 0 1 1 1 0 1 0 0 ...\n $ Fruits              : num [1:253680] 0 0 1 1 1 1 0 0 1 0 ...\n $ Veggies             : num [1:253680] 1 0 0 1 1 1 0 1 1 1 ...\n $ HvyAlcoholConsump   : num [1:253680] 0 0 0 0 0 0 0 0 0 0 ...\n $ AnyHealthcare       : num [1:253680] 1 0 1 1 1 1 1 1 1 1 ...\n $ NoDocbcCost         : num [1:253680] 0 1 1 0 0 0 0 0 0 0 ...\n $ GenHlth             : num [1:253680] 5 3 5 2 2 2 3 3 5 2 ...\n $ MentHlth            : num [1:253680] 18 0 30 0 3 0 0 0 30 0 ...\n $ PhysHlth            : num [1:253680] 15 0 30 0 0 2 14 0 30 0 ...\n $ DiffWalk            : num [1:253680] 1 0 1 0 0 0 0 1 1 0 ...\n $ Sex                 : num [1:253680] 0 0 0 0 0 1 0 0 0 1 ...\n $ Age                 : num [1:253680] 9 7 9 11 11 10 9 11 9 8 ...\n $ Education           : num [1:253680] 4 6 4 3 5 6 6 4 5 4 ...\n $ Income              : num [1:253680] 3 1 8 6 4 8 7 4 1 3 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   Diabetes_binary = col_double(),\n  ..   HighBP = col_double(),\n  ..   HighChol = col_double(),\n  ..   CholCheck = col_double(),\n  ..   BMI = col_double(),\n  ..   Smoker = col_double(),\n  ..   Stroke = col_double(),\n  ..   HeartDiseaseorAttack = col_double(),\n  ..   PhysActivity = col_double(),\n  ..   Fruits = col_double(),\n  ..   Veggies = col_double(),\n  ..   HvyAlcoholConsump = col_double(),\n  ..   AnyHealthcare = col_double(),\n  ..   NoDocbcCost = col_double(),\n  ..   GenHlth = col_double(),\n  ..   MentHlth = col_double(),\n  ..   PhysHlth = col_double(),\n  ..   DiffWalk = col_double(),\n  ..   Sex = col_double(),\n  ..   Age = col_double(),\n  ..   Education = col_double(),\n  ..   Income = col_double()\n  .. )\n - attr(*, \"problems\")=<externalptr> \n```\n\n\n:::\n:::\n\n\nNext, we can check whether there are duplicated data in this dataset.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndeplicates <- data_original[duplicated(data_original), ]\nnrow(deplicates)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 24206\n```\n\n\n:::\n\n```{.r .cell-code}\n#from the output, we see that there are 24206 duplicates\n\n#then we need to exclude data that are duplicated\n#dataset data is updated here\ndata_no_duplicates <- data_original[!duplicated(data_original), ]\nnrow(data_no_duplicates)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 229474\n```\n\n\n:::\n\n```{.r .cell-code}\n#from the output, there are total 229,474 rows in the updated dataset data\n```\n:::\n\n\nNow we want to convert numeric variables to factors according to previous sorted unique values for each variable.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata <- data_no_duplicates\n\ndata$Diabetes_binary <- factor(\n  data$Diabetes_binary, \n  levels = c(0,1), \n  labels = c(\"No_diabetes\", \"Prediabetes_or_diabetes\")\n)\n\ndata$HighBP <- factor(\n  data$HighBP,\n  levels = c(0,1), \n  labels = c(\"No_high_BP\", \"High_BP\")\n)\n\ndata$HighChol <- factor(\n  data$HighChol,\n  levels = c(0,1), \n  labels = c(\"No_high_cholesterol\", \"High_cholesterol\")\n)\n\ndata$CholCheck <- factor(\n  data$CholCheck,\n  levels = c(0,1), \n  labels = c(\"No_cholesterol_check\", \"Cholesterol_check\")\n)\n\ndata$Smoker <- factor(\n  data$Smoker,\n  levels = c(0,1), \n  labels = c(\"No\", \"Yes\")\n)\n\ndata$Stroke <- factor(\n  data$Stroke,\n  levels = c(0,1), \n  labels = c(\"No\", \"Yes\")\n)\n\ndata$HeartDiseaseorAttack <- factor(\n  data$HeartDiseaseorAttack,\n  levels = c(0,1), \n  labels = c(\"No\", \"Yes\")\n)\n\ndata$PhysActivity <- factor(\n  data$PhysActivity,\n  levels = c(0,1), \n  labels = c(\"No\", \"Yes\")\n)\n\ndata$Fruits <- factor(\n  data$Fruits,\n  levels = c(0,1), \n  labels = c(\"No\", \"Yes\")\n)\n\ndata$Veggies <- factor(\n  data$Veggies,\n  levels = c(0,1), \n  labels = c(\"No\", \"Yes\")\n)\n\ndata$HvyAlcoholConsump <- factor(\n  data$HvyAlcoholConsump,\n  levels = c(0,1), \n  labels = c(\"No\", \"Yes\")\n)\n\ndata$AnyHealthcare <- factor(\n  data$AnyHealthcare,\n  levels = c(0,1), \n  labels = c(\"No\", \"Yes\")\n)\n\ndata$NoDocbcCost <- factor(\n  data$NoDocbcCost,\n  levels = c(0,1), \n  labels = c(\"No\", \"Yes\")\n)\n\ndata$GenHlth <- factor(\n  data$GenHlth,\n  levels = c(1, 2, 3, 4, 5), \n  labels = c(\"Excellent\", \"Very_good\", \"Good\", \"Fair\", \"Poor\")\n)\n\ndata$DiffWalk <- factor(\n  data$DiffWalk,\n  levels = c(0,1), \n  labels = c(\"No\", \"Yes\")\n)\n\ndata$Sex <- factor(\n  data$Sex,\n  levels = c(0,1), \n  labels = c(\"Female\", \"Male\")\n)\n\ndata$Age <- factor(\n  data$Age,\n  levels = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13), \n  labels = c(\"18-24\", \"25-29\", \"30-34\", \"35-39\", \"40-44\", \"45-49\", \"50-54\", \"55-59\", \"60-64\", \"65-69\", \"70-74\", \"75-79\", \"80_or_older\")\n)\n\ndata$Education <- factor(\n  data$Education,\n  levels = c(1, 2, 3, 4, 5, 6), \n  labels = c(\"Never_attended_school_or_only_kindergarten\", \"Elementary\", \"Some_high_school\", \"High school_graduate\", \"Some_college_or_technical_school\", \"College_graduate\")\n)\n\ndata$Income <- factor(\n  data$Income,\n  levels = c(1, 2, 3, 4, 5, 6, 7, 8), \n  labels = c(\"Less_than_10K\", \"10K_to_less_than_15K\", \"15K_to_Less_than_20K\", \"20K_to_less_than_25K\", \"25K_to_less_than_35K\", \"35K_to_less_than_50k\", \"50k_to_less_than_75k\", \"75k_or_more\")\n)\n\nstr(data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ntibble [229,474 × 22] (S3: tbl_df/tbl/data.frame)\n $ Diabetes_binary     : Factor w/ 2 levels \"No_diabetes\",..: 1 1 1 1 1 1 1 1 2 1 ...\n $ HighBP              : Factor w/ 2 levels \"No_high_BP\",\"High_BP\": 2 1 2 2 2 2 2 2 2 1 ...\n $ HighChol            : Factor w/ 2 levels \"No_high_cholesterol\",..: 2 1 2 1 2 2 1 2 2 1 ...\n $ CholCheck           : Factor w/ 2 levels \"No_cholesterol_check\",..: 2 1 2 2 2 2 2 2 2 2 ...\n $ BMI                 : num [1:229474] 40 25 28 27 24 25 30 25 30 24 ...\n $ Smoker              : Factor w/ 2 levels \"No\",\"Yes\": 2 2 1 1 1 2 2 2 2 1 ...\n $ Stroke              : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n $ HeartDiseaseorAttack: Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 2 1 ...\n $ PhysActivity        : Factor w/ 2 levels \"No\",\"Yes\": 1 2 1 2 2 2 1 2 1 1 ...\n $ Fruits              : Factor w/ 2 levels \"No\",\"Yes\": 1 1 2 2 2 2 1 1 2 1 ...\n $ Veggies             : Factor w/ 2 levels \"No\",\"Yes\": 2 1 1 2 2 2 1 2 2 2 ...\n $ HvyAlcoholConsump   : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n $ AnyHealthcare       : Factor w/ 2 levels \"No\",\"Yes\": 2 1 2 2 2 2 2 2 2 2 ...\n $ NoDocbcCost         : Factor w/ 2 levels \"No\",\"Yes\": 1 2 2 1 1 1 1 1 1 1 ...\n $ GenHlth             : Factor w/ 5 levels \"Excellent\",\"Very_good\",..: 5 3 5 2 2 2 3 3 5 2 ...\n $ MentHlth            : num [1:229474] 18 0 30 0 3 0 0 0 30 0 ...\n $ PhysHlth            : num [1:229474] 15 0 30 0 0 2 14 0 30 0 ...\n $ DiffWalk            : Factor w/ 2 levels \"No\",\"Yes\": 2 1 2 1 1 1 1 2 2 1 ...\n $ Sex                 : Factor w/ 2 levels \"Female\",\"Male\": 1 1 1 1 1 2 1 1 1 2 ...\n $ Age                 : Factor w/ 13 levels \"18-24\",\"25-29\",..: 9 7 9 11 11 10 9 11 9 8 ...\n $ Education           : Factor w/ 6 levels \"Never_attended_school_or_only_kindergarten\",..: 4 6 4 3 5 6 6 4 5 4 ...\n $ Income              : Factor w/ 8 levels \"Less_than_10K\",..: 3 1 8 6 4 8 7 4 1 3 ...\n```\n\n\n:::\n:::\n\n\n## Split Data\n\nSplit this data into a training and test set. Before modeling, let’s scale and centralized data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(11)\n\ntrainIndex <- createDataPartition(data$Diabetes_binary, p = .7,\n                                  list = FALSE,\n                                  times = 1)\n\ntrain_data <-  data[trainIndex, ]\ntest_data <- data[-trainIndex, ]\n\n#check the dimensions of our training data and testing data frame\ndim(train_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 160632     22\n```\n\n\n:::\n\n```{.r .cell-code}\ndim(test_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 68842    22\n```\n\n\n:::\n:::\n\n\n## Logistic Regression Models\n\nI use preprocessed data (train_transformed) as my train data here in logistic regression models.\n\n### Logistic regression Model 1\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(11)\n\ntrctrl <- trainControl(method = \"cv\", number = 5)\n\nlogistic_M1_fit <- train(Diabetes_binary ~ ., \n                         data = train_data, \n                         method = \"glm\",\n                         family=\"binomial\",\n                         trControl=trctrl)\n```\n:::\n\n\n### Logistic regression Model 2\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(11)\n\nlogistic_M2_fit <- train(Diabetes_binary ~ . - NoDocbcCost - Fruits - AnyHealthcare + PhysHlth:GenHlth + PhysHlth:DiffWalk + GenHlth:DiffWalk + Income:Education + Income:GenHlth + Income:DiffWalk,\n                         data = train_data, \n                         method = \"glm\",\n                         family=\"binomial\",\n                         trControl=trctrl)\n```\n:::\n\n\n### Logistic regression Model 3\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(11)\n\nlogistic_M3_fit <- train(Diabetes_binary ~ GenHlth + HighBP + DiffWalk + BMI + HighChol + Age + HeartDiseaseorAttack + PhysHlth + Income + PhysActivity, \n                         data = train_data, \n                         method = \"glm\",\n                         family=\"binomial\",\n                         trControl=trctrl)\n```\n:::\n\n\n### Models comparison\n\nObtain log-loss value for each model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#use logLoss as our metric for logistic M1 model:\n#get predicted values corresponding to the probabilities that each observation in test data belongs to \"Prediabetes_or_diabetes\"\npredicted_prob_M1 <- predict(logistic_M1_fit,\n                             newdata = select(test_data, -Diabetes_binary), type='prob')\n\n#convert variable Diabetes_binary from factor to numeric in order to use logLoss function\nlog_loss_logistic_M1 <- logLoss(as.numeric(as.character(test_data$Diabetes_binary) == \"Prediabetes_or_diabetes\"),\n                                predicted_prob_M1$Prediabetes_or_diabetes)\n\n#use logLoss  as our metric for logistic M2 model:\n\npredicted_prob_M2 <- predict(logistic_M2_fit,\n                             newdata = select(test_data, -Diabetes_binary), type='prob')\n\nlog_loss_logistic_M2 <- logLoss(as.numeric(as.character(test_data$Diabetes_binary) == \"Prediabetes_or_diabetes\"),\n                                predicted_prob_M2$Prediabetes_or_diabetes)\n\n#use logLoss as our metric for logistic M3 model:\n\npredicted_prob_M3 <- predict(logistic_M3_fit,\n                             newdata = select(test_data, -Diabetes_binary), type='prob')\n\nlog_loss_logistic_M3 <- logLoss(as.numeric(as.character(test_data$Diabetes_binary) == \"Prediabetes_or_diabetes\"),\n                                predicted_prob_M3$Prediabetes_or_diabetes)\n\n#list of log-loss values obtained from each logistic model\nlist(log_loss_logistic_M1 = log_loss_logistic_M1, log_loss_logistic_M2 = log_loss_logistic_M2, log_loss_logistic_M3 = log_loss_logistic_M3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$log_loss_logistic_M1\n[1] 0.3466268\n\n$log_loss_logistic_M2\n[1] 0.3462027\n\n$log_loss_logistic_M3\n[1] 0.3497698\n```\n\n\n:::\n:::\n\n\nFrom the result after using logLoss(), we can see that all 3 logistic models have similar log-loss values (between 0.34 to 0.35) when using training data. Therefore, considering the complexity of models, logistic_M3_fit could be chosen as the best model among these three models since it is the simplest model among them.\n\n## Classification Tree\n\nWe do not need to use preprocessed data for tree modeling. Therefore, I use train data without preprocessing here.\n\nNow, we can fit a classification tree with a grid of values for the complexity parameter (cp). cp is a tuning parameter used in CART (Classification and Regression Trees) to control the complexity of the decision tree model. It represents the cost of adding another predictor split to the tree. A larger cp value results in a simpler tree (fewer splits), while a smaller cp value allows the tree to be more complex (more splits).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(11)\n\nclassification_fit <- train(Diabetes_binary ~ .,\n                            data = train_data,\n                            method = \"rpart\",\n                            metric = \"logLoss\",\n                            trControl = trainControl(method = \"cv\", \n                                                     number = 5,\n                                                     classProbs = TRUE,\n                                                     summaryFunction = mnLogLoss),\n                            tuneGrid = expand.grid(cp = seq(0, 0.1, by = 0.001)))\nclassification_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCART \n\n160632 samples\n    21 predictor\n     2 classes: 'No_diabetes', 'Prediabetes_or_diabetes' \n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 128507, 128505, 128505, 128505, 128506 \nResampling results across tuning parameters:\n\n  cp     logLoss  \n  0.000  0.4696247\n  0.001  0.3821432\n  0.002  0.3829163\n  0.003  0.3829226\n  0.004  0.4277851\n  0.005  0.4277851\n  0.006  0.4277851\n  0.007  0.4277851\n  0.008  0.4277851\n  0.009  0.4277851\n  0.010  0.4277851\n  0.011  0.4277851\n  0.012  0.4277851\n  0.013  0.4277851\n  0.014  0.4277851\n  0.015  0.4277851\n  0.016  0.4277851\n  0.017  0.4277851\n  0.018  0.4277851\n  0.019  0.4277851\n  0.020  0.4277851\n  0.021  0.4277851\n  0.022  0.4277851\n  0.023  0.4277851\n  0.024  0.4277851\n  0.025  0.4277851\n  0.026  0.4277851\n  0.027  0.4277851\n  0.028  0.4277851\n  0.029  0.4277851\n  0.030  0.4277851\n  0.031  0.4277851\n  0.032  0.4277851\n  0.033  0.4277851\n  0.034  0.4277851\n  0.035  0.4277851\n  0.036  0.4277851\n  0.037  0.4277851\n  0.038  0.4277851\n  0.039  0.4277851\n  0.040  0.4277851\n  0.041  0.4277851\n  0.042  0.4277851\n  0.043  0.4277851\n  0.044  0.4277851\n  0.045  0.4277851\n  0.046  0.4277851\n  0.047  0.4277851\n  0.048  0.4277851\n  0.049  0.4277851\n  0.050  0.4277851\n  0.051  0.4277851\n  0.052  0.4277851\n  0.053  0.4277851\n  0.054  0.4277851\n  0.055  0.4277851\n  0.056  0.4277851\n  0.057  0.4277851\n  0.058  0.4277851\n  0.059  0.4277851\n  0.060  0.4277851\n  0.061  0.4277851\n  0.062  0.4277851\n  0.063  0.4277851\n  0.064  0.4277851\n  0.065  0.4277851\n  0.066  0.4277851\n  0.067  0.4277851\n  0.068  0.4277851\n  0.069  0.4277851\n  0.070  0.4277851\n  0.071  0.4277851\n  0.072  0.4277851\n  0.073  0.4277851\n  0.074  0.4277851\n  0.075  0.4277851\n  0.076  0.4277851\n  0.077  0.4277851\n  0.078  0.4277851\n  0.079  0.4277851\n  0.080  0.4277851\n  0.081  0.4277851\n  0.082  0.4277851\n  0.083  0.4277851\n  0.084  0.4277851\n  0.085  0.4277851\n  0.086  0.4277851\n  0.087  0.4277851\n  0.088  0.4277851\n  0.089  0.4277851\n  0.090  0.4277851\n  0.091  0.4277851\n  0.092  0.4277851\n  0.093  0.4277851\n  0.094  0.4277851\n  0.095  0.4277851\n  0.096  0.4277851\n  0.097  0.4277851\n  0.098  0.4277851\n  0.099  0.4277851\n  0.100  0.4277851\n\nlogLoss was used to select the optimal model using the smallest value.\nThe final value used for the model was cp = 0.001.\n```\n\n\n:::\n:::\n\n\nFrom the output, we can see that the cp value that minimizes the chosen metric (logLoss) on the training set is 0.001. This cp value represents the optimal balance between model complexity and performance. The logLoss value at cp = 0.001 is 0.3821432.\n\n## Random Forest\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(11)\n\nrf_fit <- train(Diabetes_binary ~ .,\n                data = train_data,\n                method = \"ranger\",\n                metric = \"logLoss\",\n                num.trees = 100,\n                trControl = trainControl(method = \"cv\", \n                                         number = 3,\n                                         classProbs = TRUE,\n                                         summaryFunction = mnLogLoss),\n                tuneGrid = expand.grid(mtry = 1:sqrt(ncol(train_data)-1),\n                                       splitrule = \"extratrees\",\n                                       min.node.size = 100))\nrf_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRandom Forest \n\n160632 samples\n    21 predictor\n     2 classes: 'No_diabetes', 'Prediabetes_or_diabetes' \n\nNo pre-processing\nResampling: Cross-Validated (3 fold) \nSummary of sample sizes: 107089, 107088, 107087 \nResampling results across tuning parameters:\n\n  mtry  logLoss  \n  1     0.3881142\n  2     0.3613238\n  3     0.3527377\n  4     0.3492082\n\nTuning parameter 'splitrule' was held constant at a value of extratrees\n\nTuning parameter 'min.node.size' was held constant at a value of 100\nlogLoss was used to select the optimal model using the smallest value.\nThe final values used for the model were mtry = 4, splitrule = extratrees\n and min.node.size = 100.\n```\n\n\n:::\n:::\n\n\nFrom the output, we can see that the best tuning parameters to minimizes the chosen metric (logLoss) on the training set are mtry=4, splitrule = extratrees, and min.node.size = 100. The logLoss value at mtry=4, splitrule = extratrees, and min.node.size = 100 is 0.3492082.\n\n## Final Model Selection\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#use logLoss as our metric for classification model:\n#get predicted values corresponding to the probabilities that each observation in test data belongs to \"Prediabetes_or_diabetes\"\npredicted_prob_classification <- predict(classification_fit,\n                                         newdata = select(test_data, -Diabetes_binary),\n                                         type='prob')\n#convert variable Diabetes_binary from factor to numeric in order to use logLoss function\nlog_loss_classification <- logLoss(as.numeric(as.character(test_data$Diabetes_binary) == \"Prediabetes_or_diabetes\"),\n                                   predicted_prob_classification$Prediabetes_or_diabetes)\n\n#use logLoss as our metric for classification model:\npredicted_prob_rf <- predict(rf_fit,\n                             newdata = select(test_data, -Diabetes_binary),\n                             type='prob')\n#convert variable Diabetes_binary from factor to numeric in order to use logLoss function\nlog_loss_rf <- logLoss(as.numeric(as.character(test_data$Diabetes_binary) == \"Prediabetes_or_diabetes\"),\n                       predicted_prob_rf$Prediabetes_or_diabetes)\n\n#list of log-loss values obtained from each model\nlist(log_loss_logistic_M3 = log_loss_logistic_M3,\n     log_loss_classification = log_loss_classification,\n     log_loss_rf = log_loss_rf)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$log_loss_logistic_M3\n[1] 0.3497698\n\n$log_loss_classification\n[1] 0.3828286\n\n$log_loss_rf\n[1] 0.3518978\n```\n\n\n:::\n:::\n\n\nFrom the result, we can see that our logistic_M3 has the lowest log-loss value (0.3497698) among all the models, which means that the logistic_M3's predicted probabilities are closest to the actual observed outcomes among all models. Therefore, the logistic_M3 model overall did the best job (in terms of log-loss metric) on the test set.\n",
    "supporting": [
      "Modeling_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}