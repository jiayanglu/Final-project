---
title: "Modeling"
format: html
editor: visual
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE)
```

## Introduction

In this document, we will dive into the process of creating and evaluating predictive models for our dataset. Modeling is a crucial step in data analysis, where we apply various statistical and machine learning techniques to build models that can make predictions or understand patterns in the data. This document includes code for data preparation, model building, model evaluation, and final conclusion. 

We will use logLoss as our metric to evaluate our models here. Log loss, or logistic loss, is a metric that measures the performance of classification models by evaluating how well predicted probabilities match with actual outcomes. Unlike accuracy, which only counts the number of correct predictions, log loss penalizes incorrect predictions more if they are made with high confidence. This makes it particularly useful for binary classification tasks because it gives a more clear picture of how reliable the model's probability estimates are. It helps to ensure that the model not only predicts correctly but also assigns accurate probabilities to its predictions.

```{r}
#load packages needed
library(tidyverse)
library(caret)
library(Metrics)
library(rpart)
library(ranger)
```

```{r}
#read in csv data
data_original <- read_csv('diabetes_binary_health_indicators_BRFSS2015.csv')
str(data_original)
```

Next, we can check whether there are duplicated data in this dataset.

```{r}
deplicates <- data_original[duplicated(data_original), ]
nrow(deplicates)
#from the output, we see that there are 24206 duplicates

#then we need to exclude data that are duplicated
#dataset data is updated here
data_no_duplicates <- data_original[!duplicated(data_original), ]
nrow(data_no_duplicates)
#from the output, there are total 229,474 rows in the updated dataset data
```

Now we want to convert numeric variables to factors according to previous sorted unique values for each variable.

```{r}
data <- data_no_duplicates

data$Diabetes_binary <- factor(
  data$Diabetes_binary, 
  levels = c(0,1), 
  labels = c("No_diabetes", "Prediabetes_or_diabetes")
)

data$HighBP <- factor(
  data$HighBP,
  levels = c(0,1), 
  labels = c("No_high_BP", "High_BP")
)

data$HighChol <- factor(
  data$HighChol,
  levels = c(0,1), 
  labels = c("No_high_cholesterol", "High_cholesterol")
)

data$CholCheck <- factor(
  data$CholCheck,
  levels = c(0,1), 
  labels = c("No_cholesterol_check", "Cholesterol_check")
)

data$Smoker <- factor(
  data$Smoker,
  levels = c(0,1), 
  labels = c("No", "Yes")
)

data$Stroke <- factor(
  data$Stroke,
  levels = c(0,1), 
  labels = c("No", "Yes")
)

data$HeartDiseaseorAttack <- factor(
  data$HeartDiseaseorAttack,
  levels = c(0,1), 
  labels = c("No", "Yes")
)

data$PhysActivity <- factor(
  data$PhysActivity,
  levels = c(0,1), 
  labels = c("No", "Yes")
)

data$Fruits <- factor(
  data$Fruits,
  levels = c(0,1), 
  labels = c("No", "Yes")
)

data$Veggies <- factor(
  data$Veggies,
  levels = c(0,1), 
  labels = c("No", "Yes")
)

data$HvyAlcoholConsump <- factor(
  data$HvyAlcoholConsump,
  levels = c(0,1), 
  labels = c("No", "Yes")
)

data$AnyHealthcare <- factor(
  data$AnyHealthcare,
  levels = c(0,1), 
  labels = c("No", "Yes")
)

data$NoDocbcCost <- factor(
  data$NoDocbcCost,
  levels = c(0,1), 
  labels = c("No", "Yes")
)

data$GenHlth <- factor(
  data$GenHlth,
  levels = c(1, 2, 3, 4, 5), 
  labels = c("Excellent", "Very_good", "Good", "Fair", "Poor")
)

data$DiffWalk <- factor(
  data$DiffWalk,
  levels = c(0,1), 
  labels = c("No", "Yes")
)

data$Sex <- factor(
  data$Sex,
  levels = c(0,1), 
  labels = c("Female", "Male")
)

data$Age <- factor(
  data$Age,
  levels = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13), 
  labels = c("18-24", "25-29", "30-34", "35-39", "40-44", "45-49", "50-54", "55-59", "60-64", "65-69", "70-74", "75-79", "80_or_older")
)

data$Education <- factor(
  data$Education,
  levels = c(1, 2, 3, 4, 5, 6), 
  labels = c("Never_attended_school_or_only_kindergarten", "Elementary", "Some_high_school", "High school_graduate", "Some_college_or_technical_school", "College_graduate")
)

data$Income <- factor(
  data$Income,
  levels = c(1, 2, 3, 4, 5, 6, 7, 8), 
  labels = c("Less_than_10K", "10K_to_less_than_15K", "15K_to_Less_than_20K", "20K_to_less_than_25K", "25K_to_less_than_35K", "35K_to_less_than_50k", "50k_to_less_than_75k", "75k_or_more")
)

str(data)
```

## Split Data

Split this data into a training and test set. Before modeling, let’s scale and centralized data.

```{r}
set.seed(11)

trainIndex <- createDataPartition(data$Diabetes_binary, p = .7,
                                  list = FALSE,
                                  times = 1)

train_data <-  data[trainIndex, ]
test_data <- data[-trainIndex, ]

#check the dimensions of our training data and testing data frame
dim(train_data)
dim(test_data)
```

## Logistic Regression Models

Logistic regression is a statistical method used to predict the probability of a binary outcome. Unlike linear regression, which predicts a continuous value, logistic regression gives a probability between 0 and 1 by using a logistic function. It’s useful for binary classification tasks because it not only tells us if an event is likely to happen but also how confident we are about that prediction. This makes it a popular choice for problems where we need to understand and predict categorical outcomes based on input features. Here, we can fit logistic regression models since we have a binary classification task.

### Logistic regression Model 1

```{r}
set.seed(11)

trctrl <- trainControl(method = "cv", number = 5)

logistic_M1_fit <- train(Diabetes_binary ~ ., 
                         data = train_data, 
                         method = "glm",
                         family="binomial",
                         trControl=trctrl)
```

### Logistic regression Model 2

```{r}
set.seed(11)

logistic_M2_fit <- train(Diabetes_binary ~ . - NoDocbcCost - Fruits - AnyHealthcare + PhysHlth:GenHlth + PhysHlth:DiffWalk + GenHlth:DiffWalk + Income:Education + Income:GenHlth + Income:DiffWalk,
                         data = train_data, 
                         method = "glm",
                         family="binomial",
                         trControl=trctrl)
```

### Logistic regression Model 3

```{r}
set.seed(11)

logistic_M3_fit <- train(Diabetes_binary ~ GenHlth + HighBP + DiffWalk + BMI + HighChol + Age + HeartDiseaseorAttack + PhysHlth + Income + PhysActivity, 
                         data = train_data, 
                         method = "glm",
                         family="binomial",
                         trControl=trctrl)
```

### Models comparison

Obtain log-loss value for each model:

```{r}
#use logLoss as our metric for logistic M1 model:
#get predicted values corresponding to the probabilities that each observation in test data belongs to "Prediabetes_or_diabetes"
predicted_prob_M1 <- predict(logistic_M1_fit,
                             newdata = select(test_data, -Diabetes_binary), type='prob')

#convert variable Diabetes_binary from factor to numeric in order to use logLoss function
log_loss_logistic_M1 <- logLoss(as.numeric(as.character(test_data$Diabetes_binary) == "Prediabetes_or_diabetes"),
                                predicted_prob_M1$Prediabetes_or_diabetes)

#use logLoss  as our metric for logistic M2 model:

predicted_prob_M2 <- predict(logistic_M2_fit,
                             newdata = select(test_data, -Diabetes_binary), type='prob')

log_loss_logistic_M2 <- logLoss(as.numeric(as.character(test_data$Diabetes_binary) == "Prediabetes_or_diabetes"),
                                predicted_prob_M2$Prediabetes_or_diabetes)

#use logLoss as our metric for logistic M3 model:

predicted_prob_M3 <- predict(logistic_M3_fit,
                             newdata = select(test_data, -Diabetes_binary), type='prob')

log_loss_logistic_M3 <- logLoss(as.numeric(as.character(test_data$Diabetes_binary) == "Prediabetes_or_diabetes"),
                                predicted_prob_M3$Prediabetes_or_diabetes)

#list of log-loss values obtained from each logistic model
list(log_loss_logistic_M1 = log_loss_logistic_M1, log_loss_logistic_M2 = log_loss_logistic_M2, log_loss_logistic_M3 = log_loss_logistic_M3)
```

From the result after using logLoss(), we can see that all 3 logistic models have similar log-loss values (between 0.34 to 0.35) when using training data. Therefore, considering the complexity of models, logistic_M3_fit could be chosen as the best model among these three models since it is the simplest model among them.

## Classification Tree

A classification tree model is a type of decision tree used to categorize data into distinct classes. It splits data into branches based on feature values, with each branch representing a decision rule that leads to a classification outcome. The process continues until each branch ends in a leaf node that predicts the final class. Classification trees are particularly useful for predicting categorical responses because they provide a clear, rule-based structure that helps in understanding how different features influence the outcome. This interpretability and ability to handle various types of input data make classification trees a valuable tool for tasks where predicting categorical outcomes is essential.

Here, we can fit a classification tree since we have a binary classification task. I fit a classification tree with a grid of values for the complexity parameter (cp). cp is a tuning parameter used in CART (Classification and Regression Trees) to control the complexity of the decision tree model. It represents the cost of adding another predictor split to the tree. A larger cp value results in a simpler tree (fewer splits), while a smaller cp value allows the tree to be more complex (more splits).

```{r}
set.seed(11)

classification_fit <- train(Diabetes_binary ~ .,
                            data = train_data,
                            method = "rpart",
                            metric = "logLoss",
                            trControl = trainControl(method = "cv", 
                                                     number = 5,
                                                     classProbs = TRUE,
                                                     summaryFunction = mnLogLoss),
                            tuneGrid = expand.grid(cp = seq(0, 0.1, by = 0.001)))
classification_fit
```

From the output, we can see that the cp value that minimizes the chosen metric (logLoss) on the training set is 0.001. This cp value represents the optimal balance between model complexity and performance. The logLoss value at cp = 0.001 is 0.3821432.

## Random Forest

A random forest is an ensemble learning method that combines multiple decision trees to improve prediction accuracy and robustness. Instead of relying on a single classification tree, a random forest builds many trees using random subsets of the data and features. Each tree makes its own prediction, and the final outcome is determined by averaging the results (for regression) or by majority vote (for classification). This approach reduces overfitting and increases reliability compared to a single classification tree, making random forests especially useful when dealing with complex data and needing more stable, accurate predictions. This is why I use it here for a large complex data.

```{r}
set.seed(11)

rf_fit <- train(Diabetes_binary ~ .,
                data = train_data,
                method = "ranger",
                metric = "logLoss",
                num.trees = 100,
                trControl = trainControl(method = "cv", 
                                         number = 3,
                                         classProbs = TRUE,
                                         summaryFunction = mnLogLoss),
                tuneGrid = expand.grid(mtry = 1:sqrt(ncol(train_data)-1),
                                       splitrule = "extratrees",
                                       min.node.size = 100))
rf_fit
```

From the output, we can see that the best tuning parameters to minimizes the chosen metric (logLoss) on the training set are mtry=4, splitrule = extratrees, and min.node.size = 100. The logLoss value at mtry=4, splitrule = extratrees, and min.node.size = 100 is 0.3492082.

## Final Model Selection

```{r}
#use logLoss as our metric for classification model:
#get predicted values corresponding to the probabilities that each observation in test data belongs to "Prediabetes_or_diabetes"
predicted_prob_classification <- predict(classification_fit,
                                         newdata = select(test_data, -Diabetes_binary),
                                         type='prob')
#convert variable Diabetes_binary from factor to numeric in order to use logLoss function
log_loss_classification <- logLoss(as.numeric(as.character(test_data$Diabetes_binary) == "Prediabetes_or_diabetes"),
                                   predicted_prob_classification$Prediabetes_or_diabetes)

#use logLoss as our metric for classification model:
predicted_prob_rf <- predict(rf_fit,
                             newdata = select(test_data, -Diabetes_binary),
                             type='prob')
#convert variable Diabetes_binary from factor to numeric in order to use logLoss function
log_loss_rf <- logLoss(as.numeric(as.character(test_data$Diabetes_binary) == "Prediabetes_or_diabetes"),
                       predicted_prob_rf$Prediabetes_or_diabetes)

#list of log-loss values obtained from each model
list(log_loss_logistic_M3 = log_loss_logistic_M3,
     log_loss_classification = log_loss_classification,
     log_loss_rf = log_loss_rf)
```

From the result, we can see that our logistic_M3 has the lowest log-loss value (0.3497698) among all the models, which means that the logistic_M3's predicted probabilities are closest to the actual observed outcomes among all models. Therefore, the logistic_M3 model overall did the best job (in terms of log-loss metric) on the test set.
