---
title: "Modeling"
format: html
editor: visual
editor_options: 
  chunk_output_type: console
---

## Introduction

```{r}
#load packages needed
library(tidyverse)
library(ggplot2)
library(caret)
library(Metrics)
library(rpart)
library(randomForest)
library(ranger)
```

```{r}
#read in csv data
data_original <- read_csv('diabetes_binary_health_indicators_BRFSS2015.csv')
str(data_original)
```

Next, we can check whether there are duplicated data in this dataset.

```{r}
deplicates <- data_original[duplicated(data_original), ]
nrow(deplicates)
#from the output, we see that there are 24206 duplicates

#then we need to exclude data that are duplicated
#dataset data is updated here
data_no_duplicates <- data_original[!duplicated(data_original), ]
nrow(data_no_duplicates)
#from the output, there are total 229,474 rows in the updated dataset data
```

Now we want to convert numeric variables to factors according to previous sorted unique values for each variable.

```{r}
data <- data_no_duplicates

data$Diabetes_binary <- factor(
  data$Diabetes_binary, 
  levels = c(0,1), 
  labels = c("No diabetes", "Prediabetes or diabetes")
)

data$HighBP <- factor(
  data$HighBP,
  levels = c(0,1), 
  labels = c("No high BP", "High BP")
)

data$HighChol <- factor(
  data$HighChol,
  levels = c(0,1), 
  labels = c("No high cholesterol", "High cholesterol")
)

data$CholCheck <- factor(
  data$CholCheck,
  levels = c(0,1), 
  labels = c("No cholesterol check", "Cholesterol check")
)

data$Smoker <- factor(
  data$Smoker,
  levels = c(0,1), 
  labels = c("No", "Yes")
)

data$Stroke <- factor(
  data$Stroke,
  levels = c(0,1), 
  labels = c("No", "Yes")
)

data$HeartDiseaseorAttack <- factor(
  data$HeartDiseaseorAttack,
  levels = c(0,1), 
  labels = c("No", "Yes")
)

data$PhysActivity <- factor(
  data$PhysActivity,
  levels = c(0,1), 
  labels = c("No", "Yes")
)

data$Fruits <- factor(
  data$Fruits,
  levels = c(0,1), 
  labels = c("No", "Yes")
)

data$Veggies <- factor(
  data$Veggies,
  levels = c(0,1), 
  labels = c("No", "Yes")
)

data$HvyAlcoholConsump <- factor(
  data$HvyAlcoholConsump,
  levels = c(0,1), 
  labels = c("No", "Yes")
)

data$AnyHealthcare <- factor(
  data$AnyHealthcare,
  levels = c(0,1), 
  labels = c("No", "Yes")
)

data$NoDocbcCost <- factor(
  data$NoDocbcCost,
  levels = c(0,1), 
  labels = c("No", "Yes")
)

data$GenHlth <- factor(
  data$GenHlth,
  levels = c(1, 2, 3, 4, 5), 
  labels = c("Excellent", "Very good", "Good", "Fair", "Poor")
)

data$DiffWalk <- factor(
  data$DiffWalk,
  levels = c(0,1), 
  labels = c("No", "Yes")
)

data$Sex <- factor(
  data$Sex,
  levels = c(0,1), 
  labels = c("Female", "Male")
)

data$Age <- factor(
  data$Age,
  levels = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13), 
  labels = c("18-24", "25-29", "30-34", "35-39", "40-44", "45-49", "50-54", "55-59", "60-64", "65-69", "70-74", "75-79", "80 or older")
)

data$Education <- factor(
  data$Education,
  levels = c(1, 2, 3, 4, 5, 6), 
  labels = c("Never attended school or only kindergarten", "Elementary", "Some high school", "High school graduate", "Some college or technical school", "College graduate")
)

data$Income <- factor(
  data$Income,
  levels = c(1, 2, 3, 4, 5, 6, 7, 8), 
  labels = c("Less than $10,000", "$10,000 to less than $15,000", "$15,000 to less than $20,000", "$20,000 to less than $25,000", "$25,000 to less than $35,000", "$35,000 to less than $50,000", "$50,000 to less than $75,000", "$75,000 or more")
)

str(data)
```

## Split Data

Split this data into a training and test set. Before modeling, letâ€™s scale and centralized data.

```{r}
set.seed(11)

trainIndex <- createDataPartition(data$Diabetes_binary, p = .7,
                                  list = FALSE,
                                  times = 1)

train_data <-  data[trainIndex, ]
test_data <- data[-trainIndex, ]

#check the dimensions of our training data and testing data frame
dim(train_data)
dim(test_data)

pre_proc_values <- preProcess(train_data, method = c("center", "scale"))

#Scaling and centralizing train and test data sets.
train_transformed <- predict(pre_proc_values, train_data)
test_transformed <- predict(pre_proc_values, test_data)
```

## Logistic Regression Models

### Logistic regression Model 1

```{r}
set.seed(11)

trctrl <- trainControl(method = "cv", number = 5)

logistic_M1_fit <- train(Diabetes_binary ~ ., 
                         data = train_transformed, 
                         method = "glm",
                         family="binomial",
                         trControl=trctrl)
```

### Logistic regression Model 2

```{r}
set.seed(11)

logistic_M2_fit <- train(Diabetes_binary ~ . - NoDocbcCost - Fruits - AnyHealthcare + PhysHlth:GenHlth + PhysHlth:DiffWalk + GenHlth:DiffWalk + Income:Education + Income:GenHlth + Income:DiffWalk,
                         data = train_transformed, 
                         method = "glm",
                         family="binomial",
                         trControl=trctrl)
```

### Logistic regression Model 3

```{r}
set.seed(11)

logistic_M3_fit <- train(Diabetes_binary ~ GenHlth + HighBP + DiffWalk + BMI + HighChol + Age + HeartDiseaseorAttack + PhysHlth + Income + PhysActivity, 
                         data = train_transformed, 
                         method = "glm",
                         family="binomial",
                         trControl=trctrl)
```

### Models comparison

Obtain log-loss value for each model:

```{r}
#using logLoss as our metric for logistic M1 model:
#get predicted values corresponding to the probabilities that each observation in test data belongs to "Prediabetes or diabetes"
predicted_prob_M1 <- predict(logistic_M1_fit,
                             newdata = select(test_transformed, -Diabetes_binary), type='prob')

#convert variable Diabetes_binary from factor to numeric in order to use logLoss function
log_loss_logistic_M1 <- logLoss(as.numeric(as.character(test_transformed$Diabetes_binary) == "Prediabetes or diabetes"),
                                predicted_prob_M1$`Prediabetes or diabetes`)

#using logLoss  as our metric for logistic M2 model:

predicted_prob_M2 <- predict(logistic_M2_fit,
                             newdata = select(test_transformed, -Diabetes_binary), type='prob')

log_loss_logistic_M2 <- logLoss(as.numeric(as.character(test_transformed$Diabetes_binary) == "Prediabetes or diabetes"),
                                predicted_prob_M2$`Prediabetes or diabetes`)

#using logLoss as our metric for logistic M3 model:

predicted_prob_M3 <- predict(logistic_M3_fit,
                             newdata = select(test_transformed, -Diabetes_binary), type='prob')

log_loss_logistic_M3 <- logLoss(as.numeric(as.character(test_transformed$Diabetes_binary) == "Prediabetes or diabetes"),
                                predicted_prob_M3$`Prediabetes or diabetes`)

#list of log-loss values obtained from each logistic model
list(log_loss_logistic_M1 = log_loss_logistic_M1, log_loss_logistic_M2 = log_loss_logistic_M2, log_loss_logistic_M3 = log_loss_logistic_M3)
```

From the result after using logLoss(), we can see that all 3 logistic models have similar log-loss values (between 0.34 to 0.35). Therefore, considering the complexity of models, logistic_M3_fit could be chosen as the best model among these three models since it is the simplest model among them.

## Classification Tree

```{r}
set.seed(11)

#change the names of levels of response variable Diabetes_binary
#use data_no_duplicates which only contain one factor variable as response
data_2 <- data_no_duplicates

data_2$Diabetes_binary <- factor(
  data_2$Diabetes_binary, 
  levels = c(0,1), 
  labels = c("No_diabetes", "Prediabetes_or_diabetes")
)

#split data accordingly
trainIndex_2 <- createDataPartition(data_2$Diabetes_binary, p = .7,
                                    list = FALSE,
                                    times = 1)

train_data_2 <-  data_2[trainIndex_2, ]
test_data_2 <- data_2[-trainIndex_2, ]

#check the dimensions of our training data and testing data frame
dim(train_data_2)
dim(test_data_2)

pre_proc_values_2 <- preProcess(train_data_2, method = c("center", "scale"))

#Scaling and centralizing train and test data sets.
train_transformed_2 <- predict(pre_proc_values_2, train_data_2)
test_transformed_2 <- predict(pre_proc_values_2, test_data_2)
```

Now, we can fit a classification tree with a grid of values for the complexity parameter (cp). cp is a tuning parameter used in CART (Classification and Regression Trees) to control the complexity of the decision tree model. It represents the cost of adding another predictor split to the tree. A larger cp value results in a simpler tree (fewer splits), while a smaller cp value allows the tree to be more complex (more splits).

```{r}
set.seed(11)

classification_fit <- train(Diabetes_binary ~ .,
                            data = train_transformed_2,
                            method = "rpart",
                            metric = "logLoss",
                            trControl = trainControl(method = "cv", 
                                                     number = 5,
                                                     classProbs = TRUE,
                                                     summaryFunction = mnLogLoss),
                            tuneGrid = expand.grid(cp = seq(0, 0.1, by = 0.001)))
classification_fit
```

From the output, we can see that the cp value that minimizes the chosen metric (logLoss) on the training set is 0.001. This cp value represents the optimal balance between model complexity and performance.

## Random Forest

```{r}
set.seed(11)

rf_fit <- train(Diabetes_binary ~ .,
                data = train_transformed_2,
                method = "ranger",
                metric = "logLoss",
                num.trees = 100,
                trControl = trainControl(method = "cv", 
                                         number = 3,
                                         classProbs = TRUE,
                                         summaryFunction = mnLogLoss),
                tuneGrid = expand.grid(mtry = 1:sqrt(ncol(data_2)-1),
                                       splitrule = "extratrees",
                                       min.node.size = 100))
rf_fit
```

From the output, we can see that the best tuning parameters to minimizes the chosen metric (logLoss) on the training set are mtry=4, splitrule = extratrees, and min.node.size = 100. 

## Final Model Selection

```{r}
#using logLoss as our metric for classification model:
#get predicted values corresponding to the probabilities that each observation in test data belongs to "Prediabetes or diabetes"
predicted_prob_classification <- predict(classification_fit,
                                         newdata = select(test_transformed_2, -Diabetes_binary),
                                         type='prob')
#convert variable Diabetes_binary from factor to numeric in order to use logLoss function
log_loss_classification <- logLoss(as.numeric(as.character(test_transformed_2$Diabetes_binary) == "Prediabetes or diabetes"),
                                   predicted_prob_classification$`Prediabetes or diabetes`)

#using logLoss as our metric for classification model:
predicted_prob_rf <- predict(rf_fit,
                             newdata = select(test_transformed_2, -Diabetes_binary),
                             type='prob')
#convert variable Diabetes_binary from factor to numeric in order to use logLoss function
log_loss_rf <- logLoss(as.numeric(as.character(test_transformed_2$Diabetes_binary) == "Prediabetes or diabetes"),
                                   predicted_prob_rf$`Prediabetes or diabetes`)

#list of log-loss values obtained from each model
list(log_loss_logistic_M3 = log_loss_logistic_M3,
     log_loss_classification = log_loss_classification,
     log_loss_rf = log_loss_rf)
```

